{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHS 597: Homework 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xingyan \"David\" Wang\n",
    "\n",
    "ID: 949211534\n",
    "\n",
    "Due: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some common packages\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework requires us to use MNIST (Modified National Institute of Standards and Technology) dataset to correcly classify the hand-written numbers. \n",
    "\n",
    "There are 4 main data sets for this project. These datasets can be directly imported from keras package in Python. We loaded these 4 data sets into train_images, train_labels, test_images and test_labels using the following programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two datasets are gray-scale images of hand-drawn digits, from zero through nine. Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n",
    "There are 4 data sets: train_images, train_labels, test_images and test_labels. train_images include xxx observations and their labels are instored in train_labels. Similarly, we have test_images and test_labels\n",
    "The train.csv data set contain 42,000 observations with labels. Each labels is the truth for the number that represented. \n",
    "\n",
    "The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xingyanwang/anaconda3/lib/python3.7/site-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAEICAYAAABlM/5GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XuYHFW57/Hvy2RyTzADJAQIhEuGkCAmMiAIAd0BdvSwBR5EJKiRwz4REeWmm2y2+yiCGtCNcpUTNRDl5hYU8viACtkBQcJluBNCEhISCAwTciHkTjKzzh9dU12rnZ7pnqmuqp75fZ6nn1mrV61ea7rfWVO9alWVOecQEZH07JJ2B0REejsNxCIiKdNALCKSMg3EIiIp00AsIpIyDcQiIinTQCwikrJOB2Izc2a22cx+mESHpHcxsyuC+HJm1ifhthXbUjFlxbZzrsMH4ICDCp6bADwLbAl+TujsdSJ1RwPzg7qvASeUUbcO+COwGVgJTC2jbj9gNvAB8C5wSRl1DbgaWBs8rgGsxLqHAn8B1uTe7tLajNSfHLxPW4L3bb8y6lbF5xS05YA+5b4/3XkUie1ZwGKgFfhqma9XdfEZ1L84aHND0Id+ZdSdGvyum4H7gLoy6iq227YrN1iBvkEHLg6C51tBvm+Jv8gC4FpgAHA68D6wR4l17wJ+BwwGjg0CZ3yJdX8MPAYMAw4JAm9KiXW/Fvxx7gPsDbwKnFdi3YOBc4FTKHMgBnYPfsczgP7AT4AnS6xbNZ9TqcEa96MwtoPnvhEMEI2UPxBXY3z+M9AMjA/afgSYWWLd8cBG4Ljgd74TuFuxXX5slx2swEnA20T+4wJvlhI0QD2wHRgSee6xUoIGGAR8CNRHnvttGUHzNnBSJH9lGUHzBDA9kj+31KCJ1DmI8gfi6cATBe/BVmBsCXWr5nMqNVjjfhTGdkHZ45QxEFdrfJIbPH8UyU8G3i2x7o+AOyP5A4P3YEgJdRXbkUdXDtaNB15yQSuBl4LnS6m73Dm3MfLciyXWrQdanHNLyq1rZsOAvYLty22XYLuu1u0Or13n3GZgWYltV93nVOWqNT7bqzvCzHYrt65zbhnBQNWFur06trsyEA8mtzsetQEYkvG6bduXW7e9tjcAg83MSqzfVdX6Xne1bjWr1vhsry4ltl2NMZbJ2O7KQLwJGFrw3FByc0VZrtu2fbl122t7KLCp4D9yJVTre93VutWsWuOzvbqU2HY1xlgmY7srA/FC4LCC/7aHBc+XUvcAM4v+B/lYiXWXAH3MbEy5dZ1z64GmYPty2yXYrqt1u8Nr18wGkZuHK/W9rqrPqcpVa3y2V7fZObe23LpmdgC5g2dLitYoXrd3x3YJE9TFVk1cSO5Nv4Dyjlg+CfyU3JHS0yjviOXd5I5aDgKOobyj0jOBR8kdGR5LLvBLPSp9HrCI3BHpvYI3vtSj0hb8ruOC97I/JS4PAvYIfsfTg3pXU/6R5cx/TmToYF3wvvUH/g78nyC9Sw+OzynkVmiMC9r+H8pbNfEBMCn4nW+n9AOMiu3odl0M1onk1u5tBZ4DJkbKLgce7KRjjwR1FxNZwwecDSzsoG4dubWKm8kdJZ0aKZtE7utYsbrRdZrNRNZpAvuS+9qxb5G6Rm5t5rrg4a3TDOpO6uSDiD5WRMofBC7voN8nkFvruDV430ZHym4BbumgblV8TqUGa9yPIrH9SDuf16d6anwG5ZcEbX4A3EpkR4HcoH52B3WnBr/rZuB+IuuIFdulx7YFGxdlZtvILfe43jn3nx1uLFImM/seuYGgHzDIOdeSYNuKbamYcmK704FYREQqSxf9ERFJWSIDsZlNMbPFZva6mc1Iok2RSlNcS1wqPjVhZjXkln2cCKwCngHOcs692t72fa2f68+givZJSrOR9Wucc3uk3Y8sKjeuQbGdJVmL7SQuO3gk8LpzbjmAmd1N7gI47QZsfwbxCZucQLekMw+7e1am3YcMKyuuQbGdJVmL7SSmJvYG3orkVwXPhcxsupk1mlnjDrYn0CWRbus0rkGxLaVJYiBu73x3bz7EOTfLOdfgnGuopV8CXRLptk7jGhTbUpokBuJVwKhIfh/gnQTaFakkxbXEJomB+BlgjJntb2Z9gS8CcxNoV6SSFNcSm4ofrHPO7TSzC8jdLqgGmO2c6+kXgJEeTnEtcUrkZo3OuQeAB5JoSyQpimuJi86sExFJmQZiEZGUaSAWEUmZBmIRkZRpIBYRSZkGYhGRlCWyfE1Eeqad/3S4l286P389jRePnuOVfWzBtDC91019vbKa+c9VoHfVQ3vEIiIp00AsIpIyDcQiIinTHHEXWR//ravZY/eS6y7+9ugw3TKw1Svb78DVYXrg+f6VFt+9Nj+v9lzD77yyNS2bw/Qnfn+pV3bQJU+W3DeRjrQeP9HLXz/7Ri9/UG3+78KPbHj+6FvD9OIG/4bG3xl9VDwdrFLaIxYRSZkGYhGRlPX6qYmaQ8Z4edevNky/c/xHvLKtR+W//tftutkre+xj/lRBVz24ZUiYvvrGKV7ZUx+9M0y/sWOrVzaz+cQwvddjlb0hrPQuO05qCNP/dvNvvbL6Wn8ZWmtkQmL5jh1e2YbW/B1KJhbcrGT7Z44I0wPmv+y/5rZt5XW4CmmPWEQkZRqIRURSpoFYRCRlvXKOuOVTHw/T1952k1dWOOdVaTucv4zn/97w1TDdZ7M/13v07y8I00Pe3umV9VuTnzMe2PhUjD2U3qBm6NAwvfm4sV7ZxT/LH5v49IBNBTWL78vdtv6TXn7ezUeH6b9//3qv7KFf3RKmx91+gVd2wGULirbRU2iPWEQkZRqIRURS1iunJvotfidMP7ttlFdWX9vc7de/tMk/S2j5Jv+su9sOvCdMb2j1px9GXP9El9rUgjXpjlW/2TtMP3PETR1sWbofDH/Gy/95cH6q4pwVJ3llc0Y/HKaHjlsbS/vVRHvEIiIp00AsIpIyDcQiIinrlXPEO5veDdM3XH2GV/bDKflTl2teGuyVvXj+DUVf86o1h4Xp108Y6JW1vN/k5acefX6YXvEt/3X258WibYjEpfDOGndNyF9FbReKL+E8Z+VkL9/48CFe/uVz868zf2t/r2x4Y36J5evr/SVytT+an2/fv+hgr6A9YhGRlMU2EJvZbDNbbWavRJ6rM7OHzGxp8HNYXO2JJEFxLUkw5+JZ+GRmxwGbgN845w4NnrsGWOecm2lmM4BhzrnLOnqdoVbnPmGTO9qkomp23y1Mt6xd55W9cWd++mHhcbO9siN/9M0wPfymri1By5qH3T3POucaOt+y54orriH92I5e1P3nc272yqIXdC/0uddOC9M1n/evOrjufx3s5dcemp9XqL/pLa9s51urirbxp7efDdNNLf6VBf/3tPz8XVw3Gc1abMe2R+yc+xuwruDpU4C2W7nOAU6Nqz2RJCiuJQmVniMe4ZxrAgh+Dm9vIzObbmaNZta4g+3tbSKSJSXFNSi2pTSZOFjnnJvlnGtwzjXU0q/zCiJVQrEtpaj08rVmMxvpnGsys5HA6k5rpKxlTfHTK3d8UHxZz/izXw3T7/2ixi9sbUF6lKqIazt8vJdfc0l+7rXwKoPPRnbW/2fTOK9s7d35ywDstt6/Etqut/s3pt01kvavD1i6ETX+P6y1F20J08PnF27dM1R6j3guMC1ITwPur3B7IklQXEus4ly+dhewADjYzFaZ2bnATOBEM1sKnBjkRaqG4lqSENvUhHPurCJF6a3Xidkhly0J0+d81P+1bt1vXpg+/oxveGVDfud/fZPqUW1xvcvA/FmdO6/5wCt7cuwfwvQbOz/0yi65/NIwPeyxN72y4YPyMy9pTLIdOXJlmF6RQvtJyMTBOhGR3kwDsYhIyjQQi4ikrFdefa2rWt7fEKbXft2/6tSbc/NLg2Zc9Ruv7N+/cJqXd8/nF/mM+mHBjRFjOuVceqetx+eXrP1l7M1Ft/vXCy/28kPuyx/H6OqyM+k67RGLiKRMA7GISMo0NdFFrS8u8vJfvOI7YfqO7/3UK3vhKH+qgsi9RccPusArGvPL/EXkdy5f0b1OSq9z2JUvhOldCvazohd1H3Df04n1qRS1lj8bdUfB7FyN9fzpOu0Ri4ikTAOxiEjKNBCLiKRMc8QxqZudX4Z2wWL/FOehM/07E9x1wF/C9MKv3OiVjR31r2H64Cv8/5MtS5d3u5/Ss7z/5aO9/HdH5I9PtBbcBPTZv+avqrYv2bqLzA6XP3m6lVav7M+L8v0eQzx36Mga7RGLiKRMA7GISMo0EIuIpExzxBVgf3/By2/5vH9LsyPOzN/x+anLrvPKXvv0r8L02aNP8so2HBtXD6Wn2DnAz++6S35eeME2/04XB/zmnXy9ivaqfdFLdL7200MLSvN3cT57+We8krEXvhGme+q9brRHLCKSMg3EIiIp09REAlqa/XtLjrg+n9/2b/6XxIGW/2r5y9F/8spOPu2i/HZ/fCrOLkoPtLZlsJdP+pT56FQEwOKZHw3Tr53iL9t8cEv+ioTv3HSQVzZkfc+/w432iEVEUqaBWEQkZRqIRURSpjniCmg9doKXX3ZGfy9/6IQVYTo6J1zohnUTvfzA+xu73znpNb799zO8fH1kiViltB6fj9nVl2z1yhY15OeFJ798plc2aEr+9P0h9Pw54ULaIxYRSZkGYhGRlGlqoouswT8zaMm3IsvOjpnjlR3X/8OSX3e72xGmn1y3v1/Y2oSIx/xs9K4c1x17l1d2E/WxN7/yB/7V3+79yrVhur7Wn3b7+NPTwvRep70ae1+qmfaIRURSpoFYRCRlsQzEZjbKzOab2SIzW2hmFwbP15nZQ2a2NPg5LI72RJKi2JYkxDVHvBO41Dn3nJkNAZ41s4eArwLznHMzzWwGMAO4LKY2K67P/vt5+WXn7BWmv3/m3V7Z6YPXdKmNy5sbvPyj1+Vv8TxszoLCzSV52Y7tghscR+9ucfyAtV7ZRbcdHqYPvNW/C0btuxvDdPPxe3hldWfm7zDzzX3neWWfGegviZu7eUSY/srLU7yy3f/foH/ovuTEskfsnGtyzj0XpDcCi4C9gVOAtiNXc4BT42hPJCmKbUlC7HPEZjYamAg8BYxwzjVBLqCB4UXqTDezRjNr3MH2uLskEgvFtlRKrMvXzGwwcC9wkXPuAzPrrAoAzrlZwCyAoVbnOtk8Vn1G7+vlNxw+Mkyf+YM/e2XnfeQPXWrj0qajvPyCm/PTEXW3Pe2VDWvVdEQWVWNs9zf/z3vRibeE6ccn+Wd7Lt2+Z5g+Z9cVJbdx4TuTvPyfn8ifVTrmwt53hlxXxbZHbGa15AL1Dudc24jVbGYjg/KRwOpi9UWySrEtlRbXqgkDfg0scs5dGymaC7St4p4G3B9HeyJJUWxLEuKamjgG+DLwspm13bDtcmAm8N9mdi7wJnBGkfoiWaXYloqLZSB2zj3OP5xsGZocRxvd0Wfknl5+3ez8Mpqv7/+oV3bWkOYutXHB2/k7ez73C//qa7vf84qXr9uoeeBqkfXYHvGIPyNy2dfypxxfvWfxOCs87f7Y/iuKbvv89vwX57Mene6V1Z/jL18b0wuvnBYHnVknIpIyDcQiIinrMVdf+/Cf/TPUPrx4XZi+/KAHvLKTBmzuUhvNLfkLXR8391KvbOx3XwvTde/7Xwn9c5hE4tOyZJmXX3rG6DA97pvf9Mpe/cINJb3m2AfO9/IH37wlTNc/X/mLy/dG2iMWEUmZBmIRkZRpIBYRSVmPmSNecar/P2XJR39fUr2b3j/Qy1/36Elh2lr8VUtjr3ojTI9pfsoraympNZHK2rl8RZg+6OIVXtnnLj6ipNeo5xkvn+h52b2U9ohFRFKmgVhEJGU9Zmqi/uv+VcxO/vrhRbbs5HV4umiZph9EpBK0RywikjINxCIiKdNALCKSMg3EIiIp00AsIpIyDcQiIinTQCwikjINxCIiKdNALCKSMg3EIiIpM+eydW0lM3sPWAnsDqxJuTttstQXSK4/+znn9kignV5Bsd2pJPuSqdjO3EDcxswanXMNnW9ZeVnqC2SvP1KeLH1+6ks2aGpCRCRlGohFRFKW5YF4VtodiMhSXyB7/ZHyZOnzU18yILNzxCIivUWne8Rm5sxss5n9MIkOSe9iZlcE8eXMLNEbFSi2pZLKim3nXIcPcvcOPKjguQnAs8CW4OeEzl4nUnc0MD+o+xpwQhl164A/ApvJLQOaWkbdfsBs4APgXeCSMup+OujzBmBFqfUi9acG/d0M3AfUlVF3cvA+bQn6sF8ZdavicwrackCfct/b7jwqENtXAi8DO4Hvl9mX7sSnAVcDa4PHNQTfdkusf3HQ5oagD/3KqKvYjiG2yw5WoG/QgYuD4PlWkO9b4i+yALgWGACcDrwP7FGwzRRgMfA6MCPy/F3A74DBwLFB4Iwvsd0fA48Bw4BDgsCb0s52s4HVwCuR504AXgGaga3AsDI+uPHARuC4oN93AneXWPew4I96FfAqMA94MgiGh4Clwc9/6E8Sn1MHdcv6nEoN1rgfFYjtacBngPtpZyAuFtflxGeRdr8WvO4+wN5BrJxXYmx/Htge/J7zgceBmYrtZGO7K8F6EvA2kf+4wJulBA1QH3zoQyLPPRYNGqAGWAYcELzhLwLjgEHAh0B9ZNvflhE0bwMnRfJXthc0QVB9vCBYrwFmkBuQ1wNXl9JmUPdHwJ2R/IHB7zGkhLrfAV4M0kOC4NwG/IrgDzno1z/0p9KfUwd1y/6cSg3WuB9xxnbB695OwUBcLK7Ljc8i7T0BTI/kzwWeLDG2XwXmR2LpbuBdxXaysd2VVRPjgZdc0ErgpeD5Uuoud85tjDz3YkHdI4HXnXPLnXMfkguMU8i9gS3OuSUd1G2XmQ0D9gq277Cuc+5vwLqCp08B5gTpTcCpnbUZMT7arnNuGcGHWULdfcj9kRG8Z68CTeT2utr6M6dIfyr9ORXT5c8pA7rznnWmWFyXFZ9FeDFWrG6R2D4g6AvkYqkBGGFmu5XbrmK767oyEA8mtzsetYHcf7U46u4NvBXJrwqe6267bduXWxdghHOuKUi3AMNLrNfWdrffLzMbDUwkNz0yrK0/wc/2+lPpz6kSddNWyb4Xi+u2dtva6kq7hf3eAAw2Myuhbi25r/VtsdQ2ACcWJ4rtrg3Em4ChBc8NJTdXFEfd9oLHxdBu2/bl1u2ubr9fZjYYuBe4iNzXo1LWHFb6c6pE3bRVsu/F4rqt3ba2utJuYb+HApsK9hg7Eq3b1s9E4kSxndOVgXghcFjBf9vDgudLqXuAmUX/g3ysoO4qYFQkvw/wDrAE6GNmYzqo2y7n3HpyX3s+Vm7dQLOZjQzSNeQOeJRqYbRdMzuA3AGGJUVr+HUnkAvUO4C/kJuHW93Wn+Bne/2p9OdUTJc/pwzoznvWmWJxHUd8ejFWZt3NwDEQxtJGoNk5t7bcdhXb3VDCBHWxI8sXknvTL6C8I5ZPAj8F+gOnUXDEEugDLAf2J39QY3xQdje5o5aDyAVPOasmZgKPkjsqPZZc4Lc7uU9ugj16QOMnwHfJzV+tB/6rjN93PLklSZOCft9O6Qdh9iA35/an4P26Onj/foJ/QOOadupW9HPqpG5ZnxPZOVjX3fesNni/7gSuCtI1ncV1ufHZTrvnAYvITXXsRW5gaPfgUzux/Xtyg+844Irg9y1n1YRiO41VE8FzE8mt3dsKPAdMjJRdDjzYScceCeouJrKGDzg7CKLPkvvvswz4j0h5Hbm1ipvJHSWdGimbRO7rWLF2o+s0m4ms0wT2Jfe1Y9/gTW4CdpDbizkX+FzwPkQfj0TqLwTO7qDtqUF/N5Nb2lQXKXsQuLxIvWODtrYBrUEfv0puHm9eEECrKLJ2s9KfUwd1y/qcSg3WuB8ViO3b2omTr0Z+763txXU58VmkXSO3smdd8PDWEQd1JxWJ7d2CPu0MHncQWUes2E4mtjs9xdnMtpFb7nG9c+4/O9xYpExm9j3gEnID0SDnXEuCbSu2pWLKiW1da0JEJGVZvvqaiEivkMhAbGZTzGyxmb1uZjOSaFOk0hTXEpeKT02YWQ25gwEnkpt8fwY4yzn3anvb97V+rj+DKtonKc1G1q9xGbqvV5aUG9eg2M6SrMV2EpcdDE/tBDCztlM72w3Y/gziEzY5gW5JZx5296xMuw8ZVlZcg2I7S7IW20lMTXR0aicAZjbdzBrNrHEH2xPokki3dRrXoNiW0iQxEHd0amcu49ws51yDc66hln4JdEmk2zqNa1BsS2mSGIiLntopUsUU1xKbJAbiZ4AxZra/mfUFvgjMTaBdkUpSXEtsKn6wzjm308wuIHdRjxpgtnOuGi4AI1KU4lrilMjNGp1zDwAPJNGWSFIU1xIXnVknIpIyDcQiIinTQCwikjINxCIiKdNALCKSMg3EIiIp00AsIpIyDcQiIinTQCwikjINxCIiKUvkFGeJx+bPfyJMX33NL7yyK7/wlTDtGl9JrE8ipVr2k6PD9KKpN3pltVYTpo87f7pXNuC+pyvbsQzQHrGISMo0EIuIpKzHTE1sPeVIP79b/qtO3ewFSXenIlY35P9vXrniX1LsiUjn3r34k17+kTOvCdM7XN/iFSt7P+NM0h6xiEjKNBCLiKRMA7GISMp6zBzxO8f5/1MGHvh+PjM74c7EZZcaL+v23RqmJw9/zSubZ/58nEjaNo1q9fJ1u3QwL9zLaY9YRCRlGohFRFLWY6Ymrjj5917+6kUnpdST+NQcuJ+Xf+34/BzLhKe/5JXt9czLifRJpCObzsif/XnvadcVlFqYuuX9sV7Jw19oCNODVvo3w/YnOHom7RGLiKRMA7GISMo0EIuIpKzHzBHX2s60uxC7Pr/aUrRs67KhCfZEpH3bTvYvLfC9H+ePY9TXWuHmoTm/nOLl93z1iXg7VmW0RywikrLYBmIzm21mq83slchzdWb2kJktDX4Oi6s9kSQoriUJcU5N3AbcCPwm8twMYJ5zbqaZzQjyl8XVYOuxE8L0pP6Px/WymTF60NqiZaMebkmwJ73abSQc19Wk6UvbvPynB0Tz/pmh01acEKb3vK53T0UUim2P2Dn3N2BdwdOnAHOC9Bzg1LjaE0mC4lqSUOk54hHOuSaA4Ofw9jYys+lm1mhmjTvYXuEuiXRbSXENim0pTSYO1jnnZjnnGpxzDbX0S7s7IrFRbEspKr18rdnMRjrnmsxsJLA6zhdfefKAMD28ZmCcL52aPqP3DdOfr5tbdLsBb6z38poxTlRF4zrL+uyzt5dfOOlWL7/D5SNx0Q6/7pvX1ofpQTwVf+eqWKX3iOcC04L0NOD+CrcnkgTFtcQqzuVrdwELgIPNbJWZnQvMBE40s6XAiUFepGooriUJsU1NOOfOKlI0Oa42CvU5aGPRsm2vfaRSzVbUWz8fFKaP6edfd+rXH+yTz7z/QVJd6tXSiOusqRl/cJhuuPOVDrb0nfmHb3n5A+99MrY+9TSZOFgnItKbaSAWEUmZBmIRkZT1mKuvFRremJ3r+tfsvpuXbz49v4yn7gurvLJH638dyfX3yn5xU/4EruHNOkVUkrHyc/n4vWe35wtK/dOYpy77lzBdP3OZV6YllsVpj1hEJGUaiEVEUtZjpya21uX/xwzqYLtCrZMmhmlX41/Y+q0T8qeofriXf9rQLn3zX7z+OukGr6zw+tjvtuRf5z+Xn+aVrWvNT6kM3MX/MjfiqfxyPddu70W6b905R3v5P573k0iu1is7763jvfyOafnYbnnvzdj71lNpj1hEJGUaiEVEUqaBWEQkZVU9R7x9W36+qrVg1vTWy38WpudeMIFSXbbbr8L0LviTu1vdh2H6nRZ//vbG9z4Vpk94+CKv7CPP9/XyI//aHKZtpb987b1F+SvKjajx56HdMy931HWRLouexvzEVTcWlPanmAWrRnv5UStKPwVa8rRHLCKSMg3EIiIp00AsIpKyqp4jPuhL+dMtx//4Aq9s1BFvd+k156/On3783oP7eGW7LczP2fb98zMFNfNl9TR22EZ0dvntyz7plR3Rb0GYvnuTfzcEkUpZcnn+DjfRu2x0Zt+CKzFrfXvXaI9YRCRlGohFRFJW1VMTUfv/+4LONyrTSCp/iubA494rWvbd+ad7+XqernR3pJdoPX6il7+q4b6S6p34yhe9/OBGLVeLg/aIRURSpoFYRCRlGohFRFLWY+aIe6L97tdiIKmMH942y8sfWls81r7ddFyY3vWs9V6Z7roRD+0Ri4ikTAOxiEjKNDUh0gtN7Ovvg3V0Nt2CWz8epoev101rK0F7xCIiKYtlIDazUWY238wWmdlCM7sweL7OzB4ys6XBz2FxtCeSFMW2JCGuPeKdwKXOuUOAo4BvmNk4YAYwzzk3BpgX5EWqiWJbKi6WOWLnXBPQFKQ3mtkiYG/gFOBTwWZzgEeAy+Jos6eqsfz/xvX1/h1z93ww6d5IT4rtt+45NEzX2gsl1xv5yJowreVqlRH7HLGZjQYmAk8BI4JAbgvo4UXqTDezRjNr3MH2uLskEgvFtlRKrAOxmQ0G7gUucs59UGo959ws51yDc66hln5xdkkkFoptqaTYlq+ZWS25QL3DOfeH4OlmMxvpnGsys5HA6rja66laXGs+ozUtmVCtsV14hbWfT7g9TBcuV9vQui1MH/Ggf/PbsStfrUDvJCquVRMG/BpY5Jy7NlI0F5gWpKcB98fRnkhSFNuShLj2iI8Bvgy8bBYeBbgcmAn8t5mdC7wJnBFTeyJJUWxLxcW1auJxwIoUT46jDZE0KLYlCTrFOcO2HLEl7S5IFdtW19fLH9t/cyRX45X9Zcu+Ybp+un9j3Fak0nQ4SEQkZRqIRURSpqmJjImeWScivYP+6kVEUqaBWEQkZRqIRURSpjnilG1/eA8v3zJBi4UkHkNfeNfLf3PVP4XpW0Y9mnR3pAPaIxYRSZkGYhGRlGlqImV7/sy/GeNnf5a/UeMBlH7xbpFCO99Y6eVXHZVPn8zhCfdGOqI9YhGRlGkgFhFJmQZiEZGUaSAWEUmZBmIRkZRpIBYRSZkGYhGRlGkgFhFJmQZiEZGUaSAWEUmZOefS7oP6Qd4aAAACRElEQVTHzN4DVgK7A2tS7k6bLPUFkuvPfs65PTrfTEqh2O5Ukn3JVGxnbiBuY2aNzrmGtPsB2eoLZK8/Up4sfX7qSzZoakJEJGUaiEVEUpblgXhW2h2IyFJfIHv9kfJk6fNTXzIgs3PEIiK9RZb3iEVEegUNxCIiKcvkQGxmU8xssZm9bmYzEm57tpmtNrNXIs/VmdlDZrY0+Dksob6MMrP5ZrbIzBaa2YVp9ke6J824DtpXbGdU5gZiM6sBbgI+A4wDzjKzcQl24TZgSsFzM4B5zrkxwLwgn4SdwKXOuUOAo4BvBO9FWv2RLspAXINiO7MyNxADRwKvO+eWO+c+BO4GTkmqcefc34B1BU+fAswJ0nOAUxPqS5Nz7rkgvRFYBOydVn+kW1KNa1BsZ1kWB+K9gbci+VXBc2ka4ZxrglwAAcOT7oCZjQYmAk9loT9StizGNWQglhTb2RyIrZ3nevUaOzMbDNwLXOSc+yDt/kiXKK7bodjOyeJAvAoYFcnvA7yTUl/aNJvZSIDg5+qkGjazWnKBeodz7g9p90e6LItxDYrtTMjiQPwMMMbM9jezvsAXgbkp92kuMC1ITwPuT6JRMzPg18Ai59y1afdHuiWLcQ2K7WxwzmXuAXwWWAIsA/4j4bbvApqAHeT2Ys4FdiN3BHdp8LMuob4cS+7r60vAC8Hjs2n1R49uf56pxXXQvmI7ow+d4iwikrIsTk2IiPQqGohFRFKmgVhEJGUaiEVEUqaBWEQkZRqIRURSpoFYRCRl/x8rxtUVeKEYAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,2)\n",
    "for ii in np.arange(0,2):\n",
    "    for jj in np.arange(0,2):\n",
    "        axs[ii,jj].imshow(train_images[ii*2+jj,:,:])\n",
    "        axs[ii,jj].title.set_text(train_labels[ii*2+jj])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can check the dimension of each dataset. First the dimension of training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can see the labels (or the \"truth\") of the training data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can see the distribution of these numbers in the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'describe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cd7e6839aae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'describe'"
     ]
    }
   ],
   "source": [
    "train_labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can see the same for the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_images.shape)\n",
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start model our data, we need to do some data standarization to make sure that there is no outliers gonna influence the analysis. Additionally, we need to reshape the data into array, so that it will be easier for later analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we shown in the class, this question can be solved using nerual network approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "# make the data type into catgeorical level\n",
    "# make it 10 rows and 2 columns\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fit nerual network. Due to the large sample size and the limitation of my PC's calcualtion ability. The"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we would like to use some common machine learning techniques to perform this work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K-Nearest Neighbour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9688\n"
     ]
    }
   ],
   "source": [
    "print(knn.score(test_images, test_labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that the accuarcy for KNN methods on test datat set is 0.9688. \n",
    "One thing for KNN method is that we need to pre-specify the number of neighbours, i.e. 5 in the upper case. However, this value can influence the model performance of the predicition in the test data set. A lower value can result in prefect predicition in the training set but may results in the a bad prediction in the test data, which is usually refered to overfitting in the test data set. Next, we would like to see the influence of this value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we created a sequence of number from 5 to 100 by 10 to represent number of neighbourhood that we will consider. Additionally, we create two empty arrary train_accuracy and test_accuracy to store future accuracy for training data set and test data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = np.arange(5, 100, 10) \n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to loop over different values of K to build KNN models and evalue the accuracy for training data set and test data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different K values \n",
    "for i, k in enumerate(neighbors): \n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean') \n",
    "    knn.fit(train_images, train_labels)\n",
    "      \n",
    "    # Compute traning and test data accuracy \n",
    "    train_accuracy[i] = knn.score(train_images, train_labels) \n",
    "    test_accuracy[i] = knn.score(test_images, test_labels)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be more intuitive, we can plot the training data accuracy and testing data accuracy against the number of neighbours: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot \n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "  \n",
    "plt.legend() \n",
    "plt.xlabel('n_neighbors') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would like to use SVM (Suppor vector machine) approach to perform this job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xingyanwang/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the SVM, we can first look at the prediction accuracy on the training dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5801    1   13    9    9   18   25    4   36    7]\n",
      " [   1 6601   30   11    7   23    3   11   47    8]\n",
      " [  40   45 5415   83   57   26   57   70  146   19]\n",
      " [  20   26  153 5493   11  171   20   54  121   62]\n",
      " [   9   23   30    7 5485   15   30   14   53  176]\n",
      " [  53   23   40  167   60 4780   96   17  124   61]\n",
      " [  30   12   29    3   27   77 5707    2   28    3]\n",
      " [  12   21   57   13   52   10    6 5906   17  171]\n",
      " [  46  129   71  143   43  158   46   26 5104   85]\n",
      " [  25   19   28  105  184   44    2  195   51 5296]]\n",
      "0.9264666666666667\n"
     ]
    }
   ],
   "source": [
    "train_pred = svc.predict(train_images)\n",
    "train_prediction = confusion_matrix(train_labels, train_pred)\n",
    "# prediction = pd.DataFrame(prediction)\n",
    "print(train_prediction)\n",
    "print(np.trace(train_prediction)/np.sum(train_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy for training dataset is 0.9265. Next, we would like to see the prediction performance on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 962    0    2    1    1    4    5    3    1    1]\n",
      " [   0 1112    3    2    0    1    5    1   11    0]\n",
      " [  11   11  911   18   10    4   13   12   39    3]\n",
      " [   4    0   19  918    2   22    5   12   20    8]\n",
      " [   1    4    5    4  913    0    9    3    6   37]\n",
      " [   9    2    0   40   12  765   19    7   30    8]\n",
      " [   7    4    7    2    5   21  909    1    2    0]\n",
      " [   2    8   23    5    7    1    1  948    5   28]\n",
      " [  10   12    8   22   14   31    8   13  844   12]\n",
      " [   7    8    2   15   31   12    0   26   12  896]]\n"
     ]
    }
   ],
   "source": [
    "test_pred = svc.predict(test_images)\n",
    "prediction = confusion_matrix(test_labels, test_pred)\n",
    "# prediction = pd.DataFrame(prediction)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above matrix that the diagonal elements are the correctly classified elements, while the off diagnoal are those mis-correclty classified elements. Based on this, we can calculate the accuracy of SVM for the test data set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(prediction)/np.sum(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (Tree-based Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we would like to fit the model. We use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoded_train_labels = pd.Series(encoder.fit_transform(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "rf.fit(train_images, encoded_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we would like to see the the prediction error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.  1.9 1.  ... 4.  4.7 5.9]\n"
     ]
    }
   ],
   "source": [
    "predictions = rf.predict(test_images)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.  ... 0.  0.3 0.1]\n"
     ]
    }
   ],
   "source": [
    "errors = abs(predictions - test_labels)\n",
    "print(errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoded_train_labels = pd.Series(encoder.fit_transform(train_labels))\n",
    "# print(encoded_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fit AdaBoost model. max_depth option tells our model that we’d like our forest to be composed of trees with a single decision node and two leaves. n_estimators tells out model that we would like to build the number of trees in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=200\n",
    ")\n",
    "classifier.fit(train_images, encoded_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, as for performance criteria, we use test data prediction accuracy and test data prediction accuracy. First, let's take a look at the test data prediction accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(test_images)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 914,    0,    9,    2,    0,   40,    7,    1,    5,    2],\n",
       "       [   0, 1103,    4,    6,    1,    2,    1,    1,   16,    1],\n",
       "       [   8,    3,  907,   21,    7,    7,   23,   12,   42,    2],\n",
       "       [   1,    1,   19,  898,    3,   36,    0,    6,   33,   13],\n",
       "       [   1,    0,   10,    0,  851,    2,    4,    7,   11,   96],\n",
       "       [   7,    1,    4,   50,    4,  772,    8,    3,   23,   20],\n",
       "       [   7,    4,   13,    1,   11,   25,  876,    2,   18,    1],\n",
       "       [   1,    8,   23,   13,   10,    2,    0,  888,   13,   70],\n",
       "       [   7,    0,   10,   27,    4,   19,    3,    5,  886,   13],\n",
       "       [   4,    7,    7,   16,   64,    6,    1,   19,   21,  864]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8959"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(confusion_matrix(test_labels, predictions))/np.sum(confusion_matrix(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are using two parameters here, max_depth and n_estimators. We then, would like to investigate the influnce these two parameters. Due to the limitation of my PCs capacity, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = np.arange(2, 4) \n",
    "n_estimators_range = np.arange(100, 200, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n",
      "[100 150]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "print(max_depth_range)\n",
    "print(n_estimators_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over different max_depth values and n_estimators \n",
    "for i, j in enumerate(max_depth_range): \n",
    "    for k, l in enumerate(n_estimators_range): \n",
    "        classifier = AdaBoostClassifier(DecisionTreeClassifier(max_depth=j),n_estimators =l)\n",
    "        classifier.fit(train_images, encoded_train_labels)\n",
    "        \n",
    "        train_predictions = classifier.predict(train_images)\n",
    "        predictions = classifier.predict(test_images)\n",
    "        \n",
    "        train_accuracy[i] = np.trace(confusion_matrix(test_labels, train_predictions))/np.sum(confusion_matrix(test_labels, train_predictions))\n",
    "        test_accuracy[i] = np.trace(confusion_matrix(test_labels, predictions))/np.sum(confusion_matrix(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we used multiple machine learning and deep learning methods to predict MNIST data set. Methods that we are using include Nerual Network, KNN, SVM (Support Vector Machine), Random Forest, AdaBoost. We use predicition accuracy on both test dataset and training data set as our criteria. After fitting these models, we can summarize results as follow: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
